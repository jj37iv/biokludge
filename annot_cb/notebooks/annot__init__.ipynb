{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T13:27:37.873914Z",
     "start_time": "2017-10-10T13:27:37.815173Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import built-in modules\n",
    "import atexit\n",
    "import bisect\n",
    "import collections\n",
    "import copy\n",
    "import csv\n",
    "import datetime\n",
    "import hashlib\n",
    "import datetime\n",
    "import math\n",
    "import io\n",
    "import os\n",
    "import random\n",
    "import subprocess\n",
    "import sys\n",
    "import tempfile\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import operator\n",
    "import inspect\n",
    "import itertools\n",
    "import time\n",
    "import __main__\n",
    "import datetime\n",
    "import inspect\n",
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import gzip\n",
    "import contextlib\n",
    "import tempfile\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "from functools import reduce\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T13:27:39.929523Z",
     "start_time": "2017-10-10T13:27:39.737667Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "def pmap(f, l, n_jobs=10, verbose=5):\n",
    "    return joblib.Parallel(n_jobs=n_jobs, verbose=verbose)(joblib.delayed(f)(l_i) for l_i in l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T13:27:42.533623Z",
     "start_time": "2017-10-10T13:27:39.932608Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import custom modules\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.signal\n",
    "import scipy.cluster.hierarchy\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import pandas.plotting\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_venn\n",
    "\n",
    "import mpl_toolkits.axes_grid1\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "import HTSeq as hts\n",
    "\n",
    "import hmmlearn\n",
    "import hmmlearn.hmm\n",
    "\n",
    "import pyBigWig\n",
    "import pybedtools\n",
    "from pybedtools import BedTool\n",
    "\n",
    "import twobitreader\n",
    "\n",
    "def listify(x):\n",
    "    if callable(getattr(self, \"tolist\", None)):\n",
    "        return x.tolist()\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "#def read_regions(fp, chroms, starts, ends, f=None):\n",
    "#    fh = pyBigWig.open(fp)\n",
    "#    for chrom, start_, end_ in zip(chroms, starts, ends):\n",
    "#        # fixes pyBigWig:\n",
    "#        # RuntimeError: You must supply a chromosome, start and end position.\n",
    "#        start = int(start_)\n",
    "#        end = int(end_)\n",
    "#        if f is None:\n",
    "#            yield fh.values(chrom, start, end)\n",
    "#        else:\n",
    "#            yield f(np.array(fh.values(chrom, start, end)))\n",
    "#w    fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-18T13:45:21.752975Z",
     "start_time": "2017-07-18T14:45:21.744103+01:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-18T13:50:21.786787Z",
     "start_time": "2017-07-18T14:50:21.782918+01:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pf(id, step, suffix, prefix=''): return os.path.join(prefix, step, '%(id)s.%(step)s%(suffix)s' % locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-18T13:47:55.348102Z",
     "start_time": "2017-07-18T14:47:55.341461+01:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(os.path.expanduser('~/relmapping'))\n",
    "print('os.getcwd():', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open('workflows/config.yaml') as fh:\n",
    "    config = collections.OrderedDict([(k, v) for k, v in yaml.load(fh).items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T10:27:49.948920Z",
     "start_time": "2018-09-18T10:27:49.931654Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Color-blindness safe colors from http://www.nature.com/nmeth/journal/v8/n6/full/nmeth.1618.html\n",
    "# http://jfly.iam.u-tokyo.ac.jp/color/\n",
    "BLACK   = '#000000' # 0,0,0\n",
    "ORANGE  = '#e69f00' # 230,159,0\n",
    "SKYBLUE = '#56b4e9' # 86,180,233\n",
    "GREEN   = '#009e73' # 0,158,115\n",
    "YELLOW  = '#f0e442' # 240,228,66\n",
    "BLUE    = '#0072b2' # 0,114,178\n",
    "RED     = '#d55e00' # 213,94,0\n",
    "PURPLE  = '#cc79a7' # 204,121,167\n",
    "\n",
    "NAMES_BED9 = ['chrom', 'start', 'end', 'name', 'score', 'strand', 'thickStart', 'thickEnd', 'itemRgb']\n",
    "NAMES_GTF = ['chrom', 'source', 'feature', 'start', 'end', 'score', 'strand', 'frame', 'attribute']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_gffbed(fp,\n",
    "                 chrom, start, end, name='', attr=None, score=0, strand='.', \n",
    "                 thickStart=None, thickEnd=None, itemRgb=BLUE, \n",
    "                 trackline='#track gffTags=on', v=False):\n",
    "    df = pd.DataFrame()\n",
    "    def force_iter(l_):\n",
    "        try:\n",
    "            l = list(l_)\n",
    "            if (len(l) > 1) and not(type(l_) is str):\n",
    "                return l\n",
    "            else:\n",
    "                return list(itertools.repeat(l_, len(df)))\n",
    "        except TypeError:\n",
    "            return list(itertools.repeat(l_, len(df)))\n",
    "    #return list(l) if hasattr(l, '__iter__') else list(itertools.repeat(l, len(df)))\n",
    "    df['chrom'] = list(chrom)\n",
    "    df['start'] = force_iter(start)\n",
    "    df['end'] = force_iter(end)\n",
    "    def pack_row(ir): return (\";\".join([(\"%s=%s\" % (k, v)).replace(\" \", \"%20\") for k, v in zip(ir[1].index, ir[1])]))\n",
    "    attr_ = pd.concat([pd.DataFrame({'Name': force_iter(name)}), attr], axis=1)\n",
    "    df['name'] = list(map(pack_row, attr_.iterrows()))\n",
    "    df['score'] = force_iter(score)\n",
    "    df['strand'] = force_iter(strand)\n",
    "\n",
    "    if not(thickStart is None):\n",
    "        df['thickStart'] = force_iter(thickStart)       \n",
    "    else:\n",
    "        df['thickStart'] = df['start'].copy().tolist()\n",
    "\n",
    "    if not(thickEnd is None):\n",
    "        df['thickEnd'] = force_iter(thickEnd)\n",
    "    else:\n",
    "        df['thickEnd'] = df['end'].copy().tolist()\n",
    "\n",
    "    df['itemRgb'] = force_iter(itemRgb)\n",
    "    with open(fp, 'w') as fh:\n",
    "        print(trackline, file=fh)\n",
    "        df.sort_values(['chrom', 'start', 'end']).to_csv(fh, sep='\\t', index=False, header=False, quoting=csv.QUOTE_NONE)\n",
    "    if v: return df\n",
    "\n",
    "def read_gffbed(fp, parse_attr=[], *args, **kwargs):\n",
    "    df = pd.read_csv(fp, sep='\\t', names=NAMES_BED9, comment='#', *args, **kwargs)\n",
    "    return df_gfftags_unpack(df, name='name') # Does not preserve attribute order...\n",
    "\n",
    "def df_gfftags_unpack(df, name='name'):\n",
    "    df_out = df.drop(name, 1)\n",
    "    df_name = pd.DataFrame(df[name].apply(hts.parse_GFF_attribute_string).tolist())\n",
    "    for col in df_name.columns:\n",
    "        df_out[col] = pd.to_numeric(df_name[col], errors='ignore', downcast='integer')\n",
    "    return df_out\n",
    "\n",
    "def read_wbgtf(fp, parse_attr=[], coords_adj=False, *args, **kwargs):\n",
    "    df = pd.read_csv(fp, sep='\\t', names=NAMES_GTF, comment='#', *args, **kwargs)\n",
    "    if coords_adj: # convert coordinates from GTF-style to BED-style\n",
    "        df['start'] = df['start'] - 1\n",
    "    if parse_attr:\n",
    "        return df_gfftags_unpack(df, name='attribute') # Does not preserve attribute order...\n",
    "    else:\n",
    "        return df\n",
    "    #df_attr_col = df_from_l_dict(map(dict_from_attr, df['attribute']))\n",
    "    #df_attr = pd.concat([df.drop('attribute', axis=1), df_attr_col], axis=1)\n",
    "    #return df_reorder_columns(df_attr, list(NAMES_GTF[:8]) + parse_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deseq2x2_greater(df_counts, prefix=None):\n",
    "    fh_inp, fp_inp = tempfile.mkstemp(dir=os.getcwd(), prefix='deseq2x2_inp_')\n",
    "    fh_out, fp_out = tempfile.mkstemp(dir=os.getcwd(), prefix='deseq2x2_out_')\n",
    "    df_counts.to_csv(fp_inp, sep='\\t')\n",
    "    !cat {fp_inp} | scripts/deseq2x2_greater.R > {fp_out}\n",
    "    #!wc -l {fp_inp}\n",
    "    #!wc -l {fp_out}\n",
    "    #!tail -n 20 {fp_out}\n",
    "    df_out = pd.read_csv(fp_out, sep='\\s+')\n",
    "    !rm {fp_inp}\n",
    "    !rm {fp_out}\n",
    "    if not(prefix is None):\n",
    "        df_out.columns = [prefix + '_' + column for column in df_out.columns]\n",
    "    return df_out\n",
    "\n",
    "def deseq2x2_greaterAbs(df_counts, prefix=None):\n",
    "    fh_inp, fp_inp = tempfile.mkstemp(dir=os.getcwd(), prefix='deseq2x2_inp_')\n",
    "    fh_out, fp_out = tempfile.mkstemp(dir=os.getcwd(), prefix='deseq2x2_out_')\n",
    "    df_counts.to_csv(fp_inp, sep='\\t')\n",
    "    !cat {fp_inp} | scripts/deseq2x2_greaterAbs.R > {fp_out}\n",
    "    #!wc -l {fp_inp}\n",
    "    #!wc -l {fp_out}\n",
    "    #!tail -n 20 {fp_out}\n",
    "    df_out = pd.read_csv(fp_out, sep='\\s+')\n",
    "    !rm {fp_inp}\n",
    "    !rm {fp_out}\n",
    "    if not(prefix is None):\n",
    "        df_out.columns = [prefix + '_' + column for column in df_out.columns]\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_gffbed(fp,\n",
    "                 chrom, start, end, name='', attr=None, score=0, strand='.', \n",
    "                 thickStart=None, thickEnd=None, itemRgb=BLUE, \n",
    "                 trackline='#track gffTags=on', v=False):\n",
    "    df = pd.DataFrame()\n",
    "    def force_iter(l_):\n",
    "        try:\n",
    "            l = list(l_)\n",
    "            if (len(l) > 1) and not(type(l_) is str):\n",
    "                return l\n",
    "            else:\n",
    "                return list(itertools.repeat(l_, len(df)))\n",
    "        except TypeError:\n",
    "            return list(itertools.repeat(l_, len(df)))\n",
    "\n",
    "    df['chrom'] = list(chrom)\n",
    "    df['start'] = force_iter(start)\n",
    "    df['end'] = force_iter(end)\n",
    "    def pack_row(ir): return (\";\".join([(\"%s=%s\" % (k, v)).replace(\" \", \"%20\") for k, v in zip(ir[1].index, ir[1])]))\n",
    "    attr_ = pd.concat([pd.DataFrame({'Name': force_iter(name)}), attr], axis=1)\n",
    "    df['name'] = list(map(pack_row, attr_.iterrows()))\n",
    "    df['score'] = force_iter(score)\n",
    "    df['strand'] = force_iter(strand)\n",
    "\n",
    "    if not(thickStart is None):\n",
    "        df['thickStart'] = force_iter(thickStart)       \n",
    "    else:\n",
    "        df['thickStart'] = df['start'].copy().tolist()\n",
    "\n",
    "    if not(thickEnd is None):\n",
    "        df['thickEnd'] = force_iter(thickEnd)\n",
    "    else:\n",
    "        df['thickEnd'] = df['end'].copy().tolist()\n",
    "\n",
    "    df['itemRgb'] = force_iter(itemRgb)\n",
    "    with open(fp, 'w') as fh:\n",
    "        print(trackline, file=fh)\n",
    "        df.sort_values(['chrom', 'start', 'end']).to_csv(fh, sep='\\t', index=False, header=False, quoting=csv.QUOTE_NONE)\n",
    "    if v: return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_regions(fp, chroms, starts, ends, v=False):\n",
    "    assert os.path.isfile(fp)\n",
    "    n_clip = 0\n",
    "    fh = pyBigWig.open(fp)\n",
    "    for chrom, start_, end_ in zip(chroms, starts, ends):\n",
    "        # fixes pyBigWig -- RuntimeError: You must supply a chromosome, start and end position.\n",
    "        start = int(start_)\n",
    "        end = int(end_)\n",
    "\n",
    "        # Clip region if necessary\n",
    "        if (0 <= start) and (end < fh.chroms(chrom)):\n",
    "            r = fh.values(chrom, start, end)#, numpy=True)\n",
    "        else:\n",
    "            r = np.zeros(end - start) # Should get the partial signal\n",
    "            n_clip += 1\n",
    "\n",
    "        yield np.array(r)\n",
    "    fh.close()\n",
    "    if v: print(n_clip, 'clipped regions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def add_unique(df_a, df_b):\n",
    "#    assert (df_a.columns == df_b.columns).all()\n",
    "#    bt_b_only = BedTool.from_dataframe(df_b).subtract(b=BedTool.from_dataframe(df_a), A=True)\n",
    "#    df_b_only = pd.read_csv(bt_b_only.fn, sep='\\t', names=df_a.columns)\n",
    "#    df_either = pd.concat([df_a, df_b_only], axis=0, ignore_index=True).sort_values(yp.NAMES_BED3).reset_index(drop=True)\n",
    "#    print('add_unique: %d peaks in merged set -- %d peaks from A plus %d of %d non-overlapping peaks from B' % (len(df_either), len(df_a), len(df_b_only), len(df_b)))\n",
    "#    return (df_either, df_b_only)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
