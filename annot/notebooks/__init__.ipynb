{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T13:27:37.873914Z",
     "start_time": "2017-10-10T13:27:37.815173Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import built-in modules\n",
    "import atexit\n",
    "import bisect\n",
    "import collections\n",
    "import copy\n",
    "import csv\n",
    "import datetime\n",
    "import hashlib\n",
    "import datetime\n",
    "import math\n",
    "import io\n",
    "import os\n",
    "import random\n",
    "import subprocess\n",
    "import sys\n",
    "import tempfile\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import operator\n",
    "import inspect\n",
    "import itertools\n",
    "import time\n",
    "import __main__\n",
    "import datetime\n",
    "import inspect\n",
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import gzip\n",
    "import contextlib\n",
    "import tempfile\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T13:27:39.929523Z",
     "start_time": "2017-10-10T13:27:39.737667Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "def pmap(f, l, n_jobs=10, verbose=5):\n",
    "    return joblib.Parallel(n_jobs=n_jobs, verbose=verbose)(joblib.delayed(f)(l_i) for l_i in l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T13:27:42.533623Z",
     "start_time": "2017-10-10T13:27:39.932608Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import custom modules\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.signal\n",
    "import scipy.cluster.hierarchy\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import pandas.plotting\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_venn\n",
    "\n",
    "import mpl_toolkits.axes_grid1\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "import HTSeq as hts\n",
    "\n",
    "import hmmlearn\n",
    "import hmmlearn.hmm\n",
    "\n",
    "import pyBigWig\n",
    "import pybedtools\n",
    "from pybedtools import BedTool\n",
    "\n",
    "import twobitreader\n",
    "\n",
    "def listify(x):\n",
    "    if callable(getattr(self, \"tolist\", None)):\n",
    "        return x.tolist()\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "#def read_regions(fp, chroms, starts, ends, f=None):\n",
    "#    fh = pyBigWig.open(fp)\n",
    "#    for chrom, start_, end_ in zip(chroms, starts, ends):\n",
    "#        # fixes pyBigWig:\n",
    "#        # RuntimeError: You must supply a chromosome, start and end position.\n",
    "#        start = int(start_)\n",
    "#        end = int(end_)\n",
    "#        if f is None:\n",
    "#            yield fh.values(chrom, start, end)\n",
    "#        else:\n",
    "#            yield f(np.array(fh.values(chrom, start, end)))\n",
    "#w    fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Custom module with code for aggregate plots, heatmaps, etc\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "sys.path.append(os.path.expanduser('~/relmapping/scripts/yarp'))\n",
    "import yarp as yp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-18T13:45:21.752975Z",
     "start_time": "2017-07-18T14:45:21.744103+01:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Color-blindness safe colors from http://www.nature.com/nmeth/journal/v8/n6/full/nmeth.1618.html\n",
    "BLACK   = '#000000' # 0,0,0\n",
    "ORANGE  = '#e69f00' # 230,159,0\n",
    "SKYBLUE = '#56b4e9' # 86,180,233\n",
    "GREEN   = '#009e73' # 0,158,115\n",
    "YELLOW  = '#f0e442' # 240,228,66\n",
    "BLUE    = '#0072b2' # 0,114,178\n",
    "RED     = '#d55e00' # 213,94,0\n",
    "PURPLE  = '#cc79a7' # 204,121,167\n",
    "\n",
    "NAMES_BED6 = ('chrom', 'start', 'end', 'name', 'score', 'strand')\n",
    "NAMES_BED9 = ('chrom', 'start', 'end', 'name', 'score', 'strand', 'thickStart', 'thickEnd', 'itemRgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-18T13:50:21.786787Z",
     "start_time": "2017-07-18T14:50:21.782918+01:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pf(id, step, suffix, prefix=''): return os.path.join(prefix, step, '%(id)s.%(step)s%(suffix)s' % locals())\n",
    "\n",
    "def lp(fp): return os.path.join(os.path.expanduser('~/lab/HTSProcessing'), fp) # Path relative to old cluster pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-18T13:47:55.348102Z",
     "start_time": "2017-07-18T14:47:55.341461+01:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(os.path.expanduser('~/relmapping'))\n",
    "print('os.getcwd():', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open('workflows/config.yaml') as fh:\n",
    "    config = collections.OrderedDict([(k, v) for k, v in yaml.load(fh).items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deseq2x2_greater(df_counts, prefix=None):\n",
    "    fh_inp, fp_inp = tempfile.mkstemp(dir=os.getcwd(), prefix='deseq2x2_inp_')\n",
    "    fh_out, fp_out = tempfile.mkstemp(dir=os.getcwd(), prefix='deseq2x2_out_')\n",
    "    df_counts.to_csv(fp_inp, sep='\\t')\n",
    "    !cat {fp_inp} | scripts/deseq2x2_greater.R > {fp_out}\n",
    "    #!wc -l {fp_inp}\n",
    "    #!wc -l {fp_out}\n",
    "    #!tail -n 20 {fp_out}\n",
    "    df_out = pd.read_csv(fp_out, sep='\\s+')\n",
    "    !rm {fp_inp}\n",
    "    !rm {fp_out}\n",
    "    if not(prefix is None):\n",
    "        df_out.columns = [prefix + '_' + column for column in df_out.columns]\n",
    "    return df_out\n",
    "\n",
    "def deseq2x2_greaterAbs(df_counts, prefix=None):\n",
    "    fh_inp, fp_inp = tempfile.mkstemp(dir=os.getcwd(), prefix='deseq2x2_inp_')\n",
    "    fh_out, fp_out = tempfile.mkstemp(dir=os.getcwd(), prefix='deseq2x2_out_')\n",
    "    df_counts.to_csv(fp_inp, sep='\\t')\n",
    "    !cat {fp_inp} | scripts/deseq2x2_greaterAbs.R > {fp_out}\n",
    "    #!wc -l {fp_inp}\n",
    "    #!wc -l {fp_out}\n",
    "    #!tail -n 20 {fp_out}\n",
    "    df_out = pd.read_csv(fp_out, sep='\\s+')\n",
    "    !rm {fp_inp}\n",
    "    !rm {fp_out}\n",
    "    if not(prefix is None):\n",
    "        df_out.columns = [prefix + '_' + column for column in df_out.columns]\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_gff(fp, chrom, source='.', feature='.', start=float('nan'), end=float('nan'), score='.', strand='.', frame='.', attr=None, v=False):\n",
    "    df = pd.DataFrame()\n",
    "    def force_iter(l): return l if hasattr(l, '__iter__') else itertools.repeat(l, len(df))\n",
    "    df['chrom'] = list(chrom)\n",
    "    df['source'] = force_iter(source)\n",
    "    df['feature'] = force_iter(feature)\n",
    "    df['start'] = force_iter(start)\n",
    "    df['end'] = force_iter(end)\n",
    "    df['score'] = force_iter(score)\n",
    "    df['strand'] = force_iter(strand)\n",
    "    df['frame'] = force_iter(frame)\n",
    "    def pack_row(ir): return (\" ; \".join([(\"%s %s\" % (k, v)) for k, v in zip(ir[1].index, ir[1])]))\n",
    "    df['attr'] = list(map(pack_row, attr.iterrows()))\n",
    "    df.to_csv(fp, sep='\\t', index=False, header=False, quoting=csv.QUOTE_NONE)\n",
    "    if v: return df\n",
    "\n",
    "def write_gffbed(fp,\n",
    "                 chrom, start, end, name='', attr=None, score=0, strand='.', \n",
    "                 thickStart=None, thickEnd=None, itemRgb=yp.BLUE, \n",
    "                 trackline='#track gffTags=on', v=False):\n",
    "    df = pd.DataFrame()\n",
    "    def force_iter(l_):\n",
    "        try:\n",
    "            l = list(l_)\n",
    "            if (len(l) > 1) and not(type(l_) is str):\n",
    "                return l\n",
    "            else:\n",
    "                return list(itertools.repeat(l_, len(df)))\n",
    "        except TypeError:\n",
    "            return list(itertools.repeat(l_, len(df)))\n",
    "    #return list(l) if hasattr(l, '__iter__') else list(itertools.repeat(l, len(df)))\n",
    "    df['chrom'] = list(chrom)\n",
    "    df['start'] = force_iter(start)\n",
    "    df['end'] = force_iter(end)\n",
    "    def pack_row(ir): return (\";\".join([(\"%s=%s\" % (k, v)).replace(\" \", \"%20\") for k, v in zip(ir[1].index, ir[1])]))\n",
    "    attr_ = pd.concat([pd.DataFrame({'Name': force_iter(name)}), attr], axis=1)\n",
    "    df['name'] = list(map(pack_row, attr_.iterrows()))\n",
    "    df['score'] = force_iter(score)\n",
    "    df['strand'] = force_iter(strand)\n",
    "\n",
    "    if not(thickStart is None):\n",
    "        df['thickStart'] = force_iter(thickStart)       \n",
    "    else:\n",
    "        df['thickStart'] = df['start'].copy().tolist()\n",
    "\n",
    "    if not(thickEnd is None):\n",
    "        df['thickEnd'] = force_iter(thickEnd)\n",
    "    else:\n",
    "        df['thickEnd'] = df['end'].copy().tolist()\n",
    "\n",
    "    df['itemRgb'] = force_iter(itemRgb)\n",
    "    with open(fp, 'w') as fh:\n",
    "        print(trackline, file=fh)\n",
    "        df.sort_values(['chrom', 'start', 'end']).to_csv(fh, sep='\\t', index=False, header=False, quoting=csv.QUOTE_NONE)\n",
    "    if v: return df\n",
    "\n",
    "def read_gffbed(fp, parse_attr=[], *args, **kwargs):\n",
    "    df = pd.read_csv(fp, sep='\\t', names=yp.NAMES_BED9, comment='#', *args, **kwargs)\n",
    "    return yp.df_gfftags_unpack(df, name='name') # Does not preserve attribute order..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regl_mode(flank_len = 0):\n",
    "    fp_ = 'annot/S2_regulatory_annotation/S2_regulatory_annotation.tsv'\n",
    "    df_regl = pd.read_csv(fp_, sep='\\t')#.query('annot == \"coding_promoter\"').reset_index(drop=True)\n",
    "    df_mode = pd.DataFrame()\n",
    "    df_mode['chrom'] = df_regl['chrom'].copy()\n",
    "    df_mode['start'] = df_regl['atac_mode'] - flank_len\n",
    "    df_mode['end'] = df_regl['atac_mode'] + flank_len + 1\n",
    "    return df_mode\n",
    "\n",
    "def addon_atac_height(df_regl):\n",
    "    fp_sites = 'annot/S1_accessible_sites/S1a_accessible_sites.tsv'\n",
    "    df_sites = pd.read_csv(fp_sites, sep='\\t')#.query('(atac_source == \"atac_wt_pe\") | (atac_source == \"atac_wt_se\")')\n",
    "    df_regl['atac_wt_mean_height'] = df_sites[['atac_%s_height' % (stage,) for stage in config['stages_wt']]].mean(axis=1)\n",
    "    df_regl['atac_wt_max_height'] = df_sites[['atac_%s_height' % (stage,) for stage in config['stages_wt']]].max(axis=1)\n",
    "    #return df_regl\n",
    "\n",
    "def addon_associated_gene_id(df_regl):\n",
    "    def promoter_gene_id_(annot_fwd, annot_rev, promoter_gene_id_fwd, promoter_gene_id_rev):\n",
    "        if annot_fwd == 'coding_promoter' and annot_rev == 'coding_promoter':\n",
    "            return ','.join([promoter_gene_id_fwd, promoter_gene_id_rev])\n",
    "        elif annot_fwd == 'coding_promoter':\n",
    "            return promoter_gene_id_fwd\n",
    "        elif annot_rev == 'coding_promoter':\n",
    "            return promoter_gene_id_rev\n",
    "        else:\n",
    "            return '.'\n",
    "\n",
    "    df_pclosest_inp = regl_mode()\n",
    "    df_pclosest_inp['gene_id'] = list(map(promoter_gene_id_, df_regl['annot_fwd'], df_regl['annot_rev'],\n",
    "        df_regl['promoter_gene_id_fwd'], df_regl['promoter_gene_id_rev']))\n",
    "    df_pclosest_inp['locus_id'] = list(map(promoter_gene_id_, df_regl['annot_fwd'], df_regl['annot_rev'],\n",
    "        df_regl['promoter_locus_id_fwd'], df_regl['promoter_locus_id_rev']))\n",
    "    df_pclosest_inp = df_pclosest_inp[df_regl['annot'] == \"coding_promoter\"].reset_index(drop=True)\n",
    "\n",
    "    def df_closest(df_a, df_b, b_prefix, **kwargs):\n",
    "        df_a_sort = df_a\n",
    "        df_b_sort = df_b.sort_values(list(df_b.columns[:3]))\n",
    "        fn_ = BedTool.from_dataframe(df_a).closest(BedTool.from_dataframe(df_b_sort).fn, **kwargs).fn\n",
    "        names_ = list(itertools.chain(df_a.columns.values,\n",
    "            ['%s_%s' % (b_prefix, col) for col in df_b.columns.values],\n",
    "            ['%s_distance' % (b_prefix)]\n",
    "        ))\n",
    "        df_ = pd.read_csv(fn_, names=names_, sep='\\t')\n",
    "        return df_[names_[len(df_a.columns):]]\n",
    "\n",
    "    df_pclosest_out = df_closest(regl_mode(), df_pclosest_inp, b_prefix='pclosest', D='ref', t='first')\n",
    "    \n",
    "    m_ = (df_regl['annot'] != 'coding_promoter') & (df_regl['associated_gene_id'] == '.')\n",
    "    print('%d non-promoters outside of outron/gene body (=no gene_id)' % (sum(m_),))\n",
    "    \n",
    "    df_regl.loc[m_, 'associated_gene_id'] = df_pclosest_out.loc[m_, 'pclosest_gene_id']\n",
    "    df_regl.loc[m_, 'associated_locus_id'] = df_pclosest_out.loc[m_, 'pclosest_locus_id']\n",
    "\n",
    "    m_ = (df_regl['annot'] != 'coding_promoter') & (df_regl['associated_gene_id'] == '.')\n",
    "    assert sum(m_) == 0 # all of putative_enhancer should have a closest promoter, and a gene_id\n",
    "    #return df_regl\n",
    "\n",
    "def addon_cv(df_regl):\n",
    "    #q_ = 'Gerstein2014_top15k_CV_rank == Gerstein2014_top15k_CV_rank'\n",
    "    q_ = '(Gerstein2014_CV == Gerstein2014_CV) & (Gerstein2014_max_rank < 15000)'\n",
    "    df_cv_inp = pd.read_csv('WS260_ce10/WS260_ce10.genes_by_CV.tsv', sep='\\t')\\\n",
    "        .query(q_)[['gene_id', 'Gerstein2014_CV']]\n",
    "\n",
    "    df_cv_out = pd.DataFrame()\n",
    "    df_cv_out['prom_CV'] = list(map(lambda x, y: np.nanmean([x, y]), \n",
    "        df_regl.merge(df_cv_inp, how='left', left_on='promoter_gene_id_fwd', right_on='gene_id')['Gerstein2014_CV'],\n",
    "        df_regl.merge(df_cv_inp, how='left', left_on='promoter_gene_id_rev', right_on='gene_id')['Gerstein2014_CV'],\n",
    "    ))\n",
    "\n",
    "    df_cv_out['enh_CV'] = df_regl.merge(df_cv_inp, how='left', left_on='associated_gene_id', right_on='gene_id')['Gerstein2014_CV']\n",
    "\n",
    "    print('%d of %d sites with CV values via promoter annotation' % (len(df_cv_out.query('prom_CV == prom_CV')), len(df_cv_out)))\n",
    "    print('%d of %d sites with CV values via \"associated gene\"' % (len(df_cv_out.query('enh_CV == enh_CV')), len(df_cv_out)))\n",
    "\n",
    "    m_ = df_regl['annot'] == 'coding_promoter'\n",
    "    df_regl.loc[m_, 'CV'] = df_cv_out.loc[m_, 'prom_CV']\n",
    "    df_regl.loc[~m_, 'CV'] = df_cv_out.loc[~m_, 'enh_CV']\n",
    "    #return df_regl\n",
    "\n",
    "def regl_addons():\n",
    "    fp_ = 'annot_Apr27/Fig2D2_regulatory_annotation_Apr27.tsv'\n",
    "    df_regl = pd.read_csv(fp_, sep='\\t')#.query('annot == \"coding_promoter\"').reset_index(drop=True)\n",
    "    addon_atac_height(df_regl)\n",
    "    addon_associated_gene_id(df_regl)\n",
    "    addon_cv(df_regl)\n",
    "    return df_regl\n",
    "\n",
    "def df_regl_():\n",
    "    fp_ = 'annot_Apr27/Fig2D2_regulatory_annotation_Apr27.tsv'\n",
    "    df_regl = pd.read_csv(fp_, sep='\\t')#.query('annot == \"coding_promoter\"').reset_index(drop=True)\n",
    "    return df_regl\n",
    "\n",
    "# Intergenic associated_gene asignments look as intended, e.g.: chrI:9,789,072-9,815,313\n",
    "#df_regl = regl_addons()\n",
    "#fp_ = 'annot/S2_regulatory_annotation/_associated_locus_id.bed'\n",
    "#df_regl[['chrom', 'start', 'end', 'associated_locus_id']].to_csv(fp_, sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summit_pos(df): return pd.Series(map(int, df[['start', 'end']].mean(axis=1)))\n",
    "\n",
    "def addon_H3K4me3(df_regl):\n",
    "    df_mean = pd.DataFrame()\n",
    "    df_max = pd.DataFrame()\n",
    "    for hmod in ['H3K4me3']:#, 'H3K4me1', 'H3K36me3', 'H3K27me3']:\n",
    "        for stage in config['stages_wt']:\n",
    "            fp_ = 'hmod_geo/tracks/%s_%s.bw' % (hmod, stage)\n",
    "            print(hmod, stage, os.path.isfile(fp_))\n",
    "            label = '%s\\n%s' % (hmod, stage)\n",
    "            df_mean[label] = list(map(np.nanmean, yp.read_regions(fp_, \n",
    "                df_regl['chrom'], summit_pos(df_regl) - 250, summit_pos(df_regl) + 250)))\n",
    "            df_max[label] = list(map(np.nanmax, yp.read_regions(fp_, \n",
    "                df_regl['chrom'], summit_pos(df_regl) - 250, summit_pos(df_regl) + 250)))\n",
    "    df_regl['H3K4me3_mean_mean'] = df_mean.mean(axis=1)\n",
    "    df_regl['H3K4me3_mean_max'] = df_mean.max(axis=1)\n",
    "    df_regl['H3K4me3_max_mean'] = df_max.mean(axis=1)\n",
    "    df_regl['H3K4me3_max_max'] = df_max.max(axis=1)\n",
    "\n",
    "def addon_H3K27me3(df_regl):\n",
    "    df_mean = pd.DataFrame()\n",
    "    df_max = pd.DataFrame()\n",
    "    for hmod in ['H3K27me3']:\n",
    "        for stage in config['stages_wt']:\n",
    "            fp_ = 'hmod_geo/tracks/%s_%s.bw' % (hmod, stage)\n",
    "            print(hmod, stage, os.path.isfile(fp_))\n",
    "            label = '%s\\n%s' % (hmod, stage)\n",
    "            df_mean[label] = list(map(np.nanmean, yp.read_regions(fp_, \n",
    "                df_regl['chrom'], summit_pos(df_regl) - 250, summit_pos(df_regl) + 250)))\n",
    "            df_max[label] = list(map(np.nanmax, yp.read_regions(fp_, \n",
    "                df_regl['chrom'], summit_pos(df_regl) - 250, summit_pos(df_regl) + 250)))\n",
    "    df_regl['H3K27me3_mean_mean'] = df_mean.mean(axis=1)\n",
    "    df_regl['H3K27me3_mean_max'] = df_mean.max(axis=1)\n",
    "    df_regl['H3K27me3_max_mean'] = df_max.mean(axis=1)\n",
    "    df_regl['H3K27me3_max_max'] = df_max.max(axis=1)\n",
    "\n",
    "def regl_Apr27():\n",
    "    fp_ = 'annot_Apr27/Fig2D2_regulatory_annotation_Apr27.tsv'\n",
    "    df_regl = pd.read_csv(fp_, sep='\\t')#.query('annot == \"coding_promoter\"').reset_index(drop=True)\n",
    "    #addon_atac_height(df_regl)\n",
    "    #addon_associated_gene_id(df_regl)\n",
    "    addon_cv(df_regl)\n",
    "    return df_regl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def locus_id_from_gene_id(l_gene_id):\n",
    "    # Attach display_id\n",
    "    def locus_id_(locus, sequence_id, gene_id):\n",
    "        if locus == locus:\n",
    "            return locus\n",
    "        elif sequence_id == sequence_id:\n",
    "            return sequence_id\n",
    "        else:\n",
    "            return gene_id\n",
    "\n",
    "    fp_geneIDs = 'wget/ftp.wormbase.org/pub/wormbase/releases/WS260/species/c_elegans/PRJNA13758/annotation/c_elegans.PRJNA13758.WS260.geneIDs.txt.gz'\n",
    "    l_ = ['gene_id', 'locus', 'sequence_id', 'status']\n",
    "    df_geneIDs = pd.read_csv(fp_geneIDs, sep=',', names=('na', 'gene_id', 'locus', 'sequence_id', 'status'))[l_]\n",
    "    df_geneIDs['display_id'] = list(map(locus_id_,df_geneIDs['locus'], df_geneIDs['sequence_id'], df_geneIDs['gene_id']))\n",
    "    df_geneIDs = df_geneIDs.set_index('gene_id')\n",
    "    for gene_id in l_gene_id:\n",
    "        yield df_geneIDs.loc[gene_id]['display_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def techreps_collapse(l_label_, include_raw=False):\n",
    "    def collapse_(s): return s if s[-1].isdigit() else s[:-1]\n",
    "    l_label = list(l_label_)\n",
    "    l_collapsed = list(sorted(set(map(collapse_, l_label))))\n",
    "    if include_raw:\n",
    "        return list(sorted(l_collapsed + l_label))\n",
    "    else:\n",
    "        return l_collapsed\n",
    "\n",
    "def techreps_retrieve(sample, config_):\n",
    "    return list(sorted(v for k, v in config_.items() if k.startswith(sample)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
